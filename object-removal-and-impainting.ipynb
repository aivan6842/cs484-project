{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object Removal and Impainting Project\n",
    "Participants:\n",
    "\n",
    "- \n",
    "- \n",
    "- Alexei Ivanov (20917576)\n",
    "\n",
    "## Description\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "Run the following cells to setup your environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install dependencies\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import scipy\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.transforms import v2\n",
    "from PIL import Image\n",
    "from copy import deepcopy\n",
    "import random\n",
    "import torch.nn as nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomness for reproducability\n",
    "seed = 484\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "#torch.use_deterministic_algorithms(True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "To train the model we decided to use OpenAI's Country211 Dataset. You can read more about the dataset here [https://github.com/openai/CLIP/blob/main/data/country211.md](https://github.com/openai/CLIP/blob/main/data/country211.md).\n",
    "\n",
    "The following cell will download the dataset which is around 20GB"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and Create Dataset\n",
    "We download the country211 dataset using pytorch, but since our objective is slightly different we need to create our own custom dataset and reload the data. We call ou custom dataset the country212 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download data\n",
    "train_dataset = torchvision.datasets.Country211(\"country211/\", split=\"train\", download=True)\n",
    "val_dataset = torchvision.datasets.Country211(\"country211/\", split=\"valid\", download=True)\n",
    "test_dataset = torchvision.datasets.Country211(\"country211/\", split=\"test\", download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# script to copy data into desired location and format\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "if not os.path.exists(\"country212\"): os.mkdir(\"country212\")\n",
    "if not os.path.exists(\"country212/train\"): os.mkdir(\"country212/train\")\n",
    "if not os.path.exists(\"country212/valid\"): os.mkdir(\"country212/valid\")\n",
    "if not os.path.exists(\"country212/test\"): os.mkdir(\"country212/test\")\n",
    "\n",
    "def flatten_dir(mode=\"train\"):\n",
    "    for dir in os.listdir(f\"country211/country211/{mode}\"):\n",
    "        for file in os.listdir(f\"country211/country211/{mode}/{dir}\"):\n",
    "            shutil.copy(f\"country211/country211/{mode}/{dir}/{file}\", f\"country212/{mode}\")\n",
    "\n",
    "# flatten_dir(\"train\")\n",
    "# flatten_dir(\"valid\")\n",
    "# flatten_dir(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert script worker and all data is present\n",
    "assert len(os.listdir(\"country212/train\")) == len(train_dataset)\n",
    "assert len(os.listdir(\"country212/valid/\")) == len(val_dataset)\n",
    "assert len(os.listdir(\"country212/test/\")) == len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STDS = torch.tensor([0.229, 0.224, 0.225])\n",
    "MEANS = torch.tensor([0.485, 0.456, 0.406])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class country212Dataset(Dataset):\n",
    "\n",
    "    def __init__(self, root_dir, transforms=None) -> None:\n",
    "        super().__init__()\n",
    "        self.root_dir = root_dir\n",
    "        self.transforms = transforms\n",
    "        self.all_imgs = sorted(os.listdir(root_dir))\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_imgs)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        This method is supposed to return training_img, correct_img. To generate this we do the following\n",
    "        \n",
    "            1) load the image\n",
    "            2) apply given transforms to the image\n",
    "            3) create a deep copy of the image which is going to be used as the ground truth image\n",
    "            4) crop the center (CENTER_CROP_OUT_SIZE, CENTER_CROP_OUT_SIZE)\n",
    "        \"\"\"\n",
    "        # (1) load image\n",
    "        img_path = os.path.join(self.root_dir, self.all_imgs[index])\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        # (2) apply given transforms to the image\n",
    "        transformed_img = self.transforms(image) if self.transforms else image\n",
    "\n",
    "        # (3) create deep copy of image\n",
    "        ground_truth_image = deepcopy(transformed_img)\n",
    "\n",
    "        # (4) crop the center\n",
    "        #       an assumption here is that the image is 256x256 which is true because we constructed the transforms\n",
    "        transformed_img[:, 78:178, 78:178] = (-MEANS / STDS)[:, None, None]\n",
    "\n",
    "        return transformed_img, ground_truth_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "transforms = v2.Compose([\n",
    "    v2.ToTensor(),\n",
    "    v2.RandomResizedCrop(size=(256, 256), antialias=True),\n",
    "    v2.RandomHorizontalFlip(p=0.5),\n",
    "    v2.ToDtype(torch.float32),\n",
    "    v2.Normalize(mean=MEANS, std=STDS),\n",
    "])\n",
    "\n",
    "my_train_dataset = country212Dataset(\"country212/train/\", transforms=transforms)\n",
    "my_val_dataset = country212Dataset(\"country212/valid/\", transforms=transforms)\n",
    "my_test_dataset = country212Dataset(\"country212/test\", transforms=transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(my_train_dataset) == len(train_dataset)\n",
    "assert len(my_val_dataset) == len(val_dataset)\n",
    "assert len(my_test_dataset) == len(test_dataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item = my_test_dataset[0]\n",
    "print(item[0].shape, item[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train image \n",
    "plt.figure()\n",
    "unnormalized = item[0] * STDS[:, None, None] + MEANS[:, None, None]\n",
    "plt.imshow(np.transpose(unnormalized, (1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ground truth image\n",
    "plt.figure()\n",
    "unnormalized = item[0] * STDS[:, None, None] + MEANS[:, None, None]\n",
    "plt.imshow(np.transpose(unnormalized, (1, 2, 0)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from now on we use our implementation of the dataset\n",
    "train_dataset, val_dataset, test_dataset = my_train_dataset, my_val_dataset, my_test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "The model we chose to go for was a conditional GAN. Instead of taking a vector of random noise the GAN generator will be conditioned on the cropped/masked image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    \"\"\"\n",
    "    This is the generator object of the GAN. The generator takes in the masked(cropped) image and fills in the masked/cropped out portion.\n",
    "\n",
    "    The architecture takes inspiration from U-net:\n",
    "\n",
    "    Encoder:\n",
    "        1) 3x256x256 -> 64x128x128\n",
    "        2) 64x128x128 -> 128x64x64\n",
    "        3) 128x64x64 -> 256x32x32\n",
    "        4) 256x32x32 -> 512x16x16\n",
    "\n",
    "    Decoder:\n",
    "        1) 512x16x16 -> 256x32x32\n",
    "        2) (256 + 256)x32x32 -> 128x64x64\n",
    "        3) (128 + 128)x64x64 -> 64x128x128\n",
    "        4) (64 + 64)x128x128 -> 64x256x256\n",
    "        5) 64x256x256 -> 3x256x256\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # encoder layers\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv4 = nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "        self.bn3 = nn.BatchNorm2d(256)\n",
    "        self.bn4 = nn.BatchNorm2d(512)\n",
    "\n",
    "        # decoder layers\n",
    "        self.convt1 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
    "        self.convt2 = nn.ConvTranspose2d(512, 128, kernel_size=2, stride=2)\n",
    "        self.convt3 = nn.ConvTranspose2d(256, 64, kernel_size=2, stride=2)\n",
    "        self.convt4 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.convt5 = nn.ConvTranspose2d(64, 3, kernel_size=1, stride=1)\n",
    "\n",
    "        # non-learnable layers\n",
    "        self.down_sample = nn.AvgPool2d(2)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # encode\n",
    "        x1 = self.relu(self.bn1(self.down_sample(self.conv1(x))))\n",
    "        x2 = self.relu(self.bn2(self.down_sample(self.conv2(x1))))\n",
    "        x3 = self.relu(self.bn3(self.down_sample(self.conv3(x2))))\n",
    "        x4 = self.relu(self.bn4(self.down_sample(self.conv4(x3))))\n",
    "\n",
    "        # decode\n",
    "        x5 = self.convt1(x4)\n",
    "        x6 = self.convt2(torch.concat([x5, x3], dim=1))\n",
    "        x7 = self.convt3(torch.concat([x6, x2], dim=1))\n",
    "        x8 = self.convt4(torch.concat([x7, x1], dim=1))\n",
    "        x9 = self.convt5(x8)\n",
    "        x10 = self.relu(x9)\n",
    "\n",
    "        return x10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Generator()\n",
    "\n",
    "# Create a random input tensor with the specified shape (3x256x256)\n",
    "input_tensor = torch.randn(1, 3, 256, 256)\n",
    "\n",
    "# Forward pass to get the output tensor\n",
    "output_tensor = model(input_tensor)\n",
    "output_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    \"\"\"\n",
    "    This is the discriminator class. The discriminator takes in an image and its job is to classify whether the image is generated(fake) \n",
    "    or real (non-generated).\n",
    "\n",
    "    The architecture of the network is the same as a simple CNN classifier\n",
    "\n",
    "        1) 3x256x256 -> 64x128x128\n",
    "        2) 64x128x128 -> 128x64x64\n",
    "        3) 128x64x64 -> 256x32x32\n",
    "        4) 256x32x32 -> 512x16x16\n",
    "        5) 512x16x16 -> 512x1x1\n",
    "        5) 512 -> 256\n",
    "        6) 256 -> 10\n",
    "        7) 10 -> 1\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # conv layers\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv4 = nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        # batch norm layers\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "        self.bn3 = nn.BatchNorm2d(256)\n",
    "        self.bn4 = nn.BatchNorm2d(512)\n",
    "\n",
    "        # linear layers\n",
    "        self.fc1 = nn.Linear(512, 256)\n",
    "        self.fc2 = nn.Linear(256, 10)\n",
    "        self.fc3 = nn.Linear(10, 1)\n",
    "\n",
    "        # non-learnable layers\n",
    "        self.relu = nn.ReLU()\n",
    "        self.down_sample = nn.AvgPool2d(2)\n",
    "        self.max_pool = nn.MaxPool2d(16)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.bn1(self.down_sample(self.conv1(x))))\n",
    "        x = self.relu(self.bn2(self.down_sample(self.conv2(x))))\n",
    "        x = self.relu(self.bn3(self.down_sample(self.conv3(x))))\n",
    "        x = self.relu(self.bn4(self.down_sample(self.conv4(x))))\n",
    "\n",
    "        x = self.max_pool(x)\n",
    "        x = x.view(x.size(0), x.size(1))\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        # return un-normalized logit, can sigmoid after if you want\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Discriminator()\n",
    "\n",
    "# Create a random input tensor with the specified shape (3x256x256)\n",
    "input_tensor = torch.randn(1, 3, 256, 256)\n",
    "\n",
    "# Forward pass to get the output tensor\n",
    "output_tensor = model(input_tensor)\n",
    "output_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator().to(device)\n",
    "discriminator = Discriminator().to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Losses, Optimizers and Schedulers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_loss = nn.BCEWithLogitsLoss()\n",
    "generator_reconstruction_loss = nn.L1Loss()\n",
    "discriminator_loss = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_optimizer = torch.optim.SGD(generator.parameters(), lr=1e-3, weight_decay=0.9)\n",
    "discriminator_optimizer = torch.optim.SGD(discriminator.parameters(), lr=1e-3, weight_decay=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_scheduler = torch.optim.lr_scheduler.ExponentialLR(generator_optimizer, gamma=0.9)\n",
    "discriminator_scheduler = torch.optim.lr_scheduler.ExponentialLR(discriminator_optimizer, gamma=0.9)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(generator, discriminator):\n",
    "\n",
    "    writer = SummaryWriter()\n",
    "    iteration = 0\n",
    "\n",
    "    print(\"Beginning training\")\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch: {epoch}\")\n",
    "        for i, data in enumerate(train_loader):\n",
    "            \n",
    "            x, y = data\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            batch_size = x.size(0)\n",
    "            \n",
    "            # ------------------------------------------------------#\n",
    "            # train Discriminator first\n",
    "            discriminator.zero_grad()\n",
    "\n",
    "            # all real batch\n",
    "            true_labels = torch.ones(batch_size, device=device)\n",
    "            \n",
    "            # forward\n",
    "            output = discriminator(y).view(-1)\n",
    "            \n",
    "            # loss\n",
    "            err_real = discriminator_loss(output, true_labels)\n",
    "\n",
    "            # backward\n",
    "            err_real.backward()\n",
    "\n",
    "            # all fake batch\n",
    "            fake_label = torch.zeros(batch_size, device=device)\n",
    "\n",
    "            # forward\n",
    "            fake_generated = generator(x)\n",
    "            output = discriminator(fake_generated.detach()).view(-1)\n",
    "\n",
    "            # loss\n",
    "            err_fake = discriminator_loss(output, fake_label)\n",
    "\n",
    "            # log loss to tensorboard\n",
    "            writer.add_scalar(\"Loss/train/discriminator\", (err_fake + err_real) / 2, iteration)\n",
    "\n",
    "            # backward\n",
    "            err_fake.backward()\n",
    "\n",
    "            # update discriminator\n",
    "            discriminator_optimizer.step()\n",
    "            # ---------------------------------------------------------#\n",
    "\n",
    "            # ---------------------------------------------------------#\n",
    "            # train generator\n",
    "            generator.zero_grad()\n",
    "\n",
    "            # labels now should be real so that we confuse discriminator that\n",
    "            # generator is creating real images\n",
    "            real_label = torch.ones(batch_size, device=device)\n",
    "            output = discriminator(fake_generated).view(-1)\n",
    "\n",
    "            # loss\n",
    "            err_generator = generator_loss(output, real_label) + generator_reconstruction_loss(fake_generated, y)\n",
    "\n",
    "            # log loss to tensorboard\n",
    "            writer.add_scalar(\"Loss/train/generator\", err_generator, iteration)\n",
    "\n",
    "            # backward\n",
    "            err_generator.backward()\n",
    "\n",
    "            # update generator\n",
    "            generator_optimizer.step()\n",
    "            # ---------------------------------------------------------#\n",
    "\n",
    "            iteration += 1\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            print(f\"Epoch {epoch} finished calculating validation metrics\")\n",
    "            val_loss = 0\n",
    "            for i, data in enumerate(val_loader):\n",
    "                x, y = data\n",
    "                x, y = x.to(device), y.to(device)\n",
    "                batch_size = x.size(0)\n",
    "\n",
    "                output = generator(x)\n",
    "                fake_label = torch.zeros(batch_size, device=device)\n",
    "                val_loss += generator_loss(discriminator(output).view(-1), fake_label) + generator_reconstruction_loss(output, y) \n",
    "            \n",
    "            writer.add_scalar(\"Loss/val\", val_loss, epoch)\n",
    "        \n",
    "        generator_scheduler.step()\n",
    "        discriminator_scheduler.step()\n",
    "\n",
    "    writer.flush()\n",
    "    writer.close()\n",
    "    return generator, discriminator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_generator, trained_discriminator = train(generator, discriminator)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item = my_test_dataset[0]\n",
    "print(item[0].shape, item[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cropped_img = item[0][None].to(device)\n",
    "with torch.no_grad():\n",
    "    output = generator(cropped_img).detach().cpu()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "unnormalized = output[0] * STDS[:, None, None] + MEANS[:, None, None]\n",
    "plt.imshow(np.transpose(unnormalized.numpy(), (1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "school",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
